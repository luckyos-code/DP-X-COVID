{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"mnist-baseline-res50.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1IOcp5rqSiazRfh-7KIZLjQhbC35Lksjt","authorship_tag":"ABX9TyNxm4gFh6xC7tLnXwofOREg"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ftwh4pLJlAX-"},"source":["# Detection of COVID-19 in X-Ray Images (COVID-19 vs. Normal)"]},{"cell_type":"code","metadata":{"id":"TLgJ3TBvws5F"},"source":["# set up path to doogle drive folder with weights\n","weights_path = '/content/drive/MyDrive/Uni/MA/model_weights/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7JW49t26X0P"},"source":["##### Notebook settings"]},{"cell_type":"code","metadata":{"id":"RBIvImfCIDZM"},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPgPDDVYD_Dc"},"source":["## Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"YoQ2ekQ1p10x"},"source":["###### Install packages"]},{"cell_type":"code","metadata":{"id":"fbLAyWnlpze2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637497973598,"user_tz":-60,"elapsed":7453,"user":{"displayName":"Lucas Lange","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07837493329680976251"}},"outputId":"265065a5-415b-4eeb-a725-c80fc41c9ed7"},"source":["!pip -q install git+https://github.com/tensorflow/privacy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.0 MB 7.9 MB/s \n","\u001b[?25h  Building wheel for tensorflow-privacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","metadata":{"id":"gAoGXtslxNAt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637497978894,"user_tz":-60,"elapsed":5302,"user":{"displayName":"Lucas Lange","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07837493329680976251"}},"outputId":"1e3b44c5-fe5a-4b1e-a9a7-4519bb390a97"},"source":["!pip -q install git+https://github.com/qubvel/classification_models.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |██████▌                         | 10 kB 33.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 20 kB 23.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 50 kB 3.6 MB/s \n","\u001b[?25h  Building wheel for image-classifiers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZLiwnMq8Kuhf"},"source":["##### Import libraries"]},{"cell_type":"code","metadata":{"id":"4geb06ZXk9sr"},"source":["import os, random\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow_datasets as tfds\n","\n","from classification_models.tfkeras import Classifiers\n","\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import Precision, Recall\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Resizing, Rescaling, RandomFlip, RandomRotation, RandomTranslation, RandomZoom\n","from tensorflow.keras.layers import Layer, Input, InputLayer, Conv2D, Activation, Add, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout, ZeroPadding2D, Flatten\n","\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import compute_rdp\n","from tensorflow_privacy.privacy.analysis.rdp_accountant import get_privacy_spent\n","from tensorflow_privacy.privacy.analysis.compute_noise_from_budget_lib import compute_noise\n","from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras_vectorized import VectorizedDPKerasAdamOptimizer\n","\n","import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.plotting as plotting\n","from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n","from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n","from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n","from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SingleAttackResult\n","from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mfzr_GlzVqf"},"source":["ResNet18, _ = Classifiers.get('resnet18')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rguGows5cq65"},"source":["##### Define constants"]},{"cell_type":"code","metadata":{"id":"i3YYpkxoc0t0"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","# Model parameters\n","BATCH_SIZE = 32\n","IMG_SHAPE = [28, 28, 3] # original dataset image size is 28x28x1\n","EPOCHS = 20\n","\n","# Set fixed random seed\n","SEED = 42\n","random.seed(SEED)\n","random_state = SEED\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_yW-sKkgixOX"},"source":["## Data\n"]},{"cell_type":"markdown","metadata":{"id":"W4-BvfKk31nU"},"source":["### Load dataset"]},{"cell_type":"code","metadata":{"id":"iTBlEfCz6tu-"},"source":["(train_ds, test_ds), ds_info = tfds.load(\n","    'mnist',\n","    split=['train', 'test'],\n","    shuffle_files=False,\n","    as_supervised=True,\n","    with_info=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZnnoc95cn42"},"source":["TRAIN_IMG_COUNT = ds_info.splits['train'].num_examples\n","TEST_IMG_COUNT = ds_info.splits['test'].num_examples\n","print(\n","    'Train: ' + str(TRAIN_IMG_COUNT) + ',',\n","    'Test: ' + str(TEST_IMG_COUNT),\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VXTNKqd1SWq1"},"source":["### Prepare for training"]},{"cell_type":"markdown","metadata":{"id":"9maiOwIFusj3"},"source":["###### Define preprocessing"]},{"cell_type":"code","metadata":{"id":"Es1bSWituVFP"},"source":["class GrayscaleToRgb(Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__(**kwargs)\n","\n","  def call(self, x):\n","    return tf.image.grayscale_to_rgb(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThfMQttGuFNI"},"source":["preprocessing = Sequential([\n","  GrayscaleToRgb(),\n","  Resizing(IMG_SHAPE[0], IMG_SHAPE[1]),\n","  Rescaling(scale=1./255)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GsOnrZz2u-wY"},"source":["###### Prepare batches"]},{"cell_type":"code","metadata":{"id":"g0sZZfKJSkaC"},"source":["def prepare_dataset(ds, cache=True, pre=False, shuffle=False, repeat=False, batch=False, augment=False, shuffle_buffer_size=1000):\n","    # give string to cache preprocessing for datasets outside of memory\n","    if cache:\n","        if isinstance(cache, str):\n","            ds = ds.cache(cache)\n","        else:\n","            ds = ds.cache()\n","    # Resize and rescale images\n","    if pre:\n","        ds = ds.map(lambda x, y: (preprocessing(x), y),\n","                    num_parallel_calls=AUTOTUNE)\n","    # shuffle dataset\n","    if shuffle:\n","        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n","    # repeat set forever or once\n","    if repeat:\n","        ds = ds.repeat()\n","    else:\n","        ds = ds.repeat(1)\n","    # batch dataset\n","    if batch:\n","        if str(batch) == '1':\n","            ds = ds.batch(batch).prefetch(buffer_size=AUTOTUNE)\n","        else:\n","            ds = ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n","\n","    return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8dlSaWrXL8k"},"source":["# batch datasets\n","train_batched = prepare_dataset(train_ds,\n","                                pre=True,\n","                                shuffle=True,\n","                                repeat=True,\n","                                batch=True,\n","                                cache='./data.tfcache')\n","\n","test_batched = prepare_dataset(test_ds,\n","                               pre=True,\n","                               shuffle=False,\n","                               repeat=False,\n","                               batch=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzXANnNilZX4"},"source":["# set for attack on train set\n","train_attack_data = prepare_dataset(train_ds,\n","                                    pre=True,\n","                                    shuffle=False,\n","                                    repeat=False,\n","                                    batch=1,\n","                                    cache=False)\n","\n","# set for attack on test set\n","test_attack_data = prepare_dataset(test_ds,\n","                                   pre=True,\n","                                   shuffle=False,\n","                                   repeat=False,\n","                                   batch=1,\n","                                   cache=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mxytOEw8X0P1"},"source":["### Visualization"]},{"cell_type":"markdown","metadata":{"id":"K9u-ej3xUxss"},"source":["###### Images from a training batch"]},{"cell_type":"code","metadata":{"id":"I813txxcXnw3"},"source":["def show_batch(image_batch, label_batch):\n","    plt.figure(figsize=(5,5))\n","    img_num = 9 if BATCH_SIZE > 9 else BATCH_SIZE\n","    for n in range(img_num):\n","        ax = plt.subplot(np.log(img_num) / np.log(2), np.log(img_num) / np.log(2), n+1)\n","        plt.imshow(image_batch[n])\n","        plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEUTa0MkYbkd"},"source":["image_batch, label_batch = next(iter(train_batched))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wqn6CTqeYfxP"},"source":["show_batch(image_batch.numpy(), label_batch.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krill9Wz9Gnj"},"source":["## Tuning"]},{"cell_type":"markdown","metadata":{"id":"XCdKpgaf9kpt"},"source":["### Learning rate decay"]},{"cell_type":"code","metadata":{"id":"2VsbJE9M9LWm"},"source":["learning_rate_decay = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.1, min_lr=1e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nN80mZdw0qxH"},"source":["## ResNet18\n","[Paper](https://scholar.google.com/scholar?cluster=9281510746729853742&hl=en&as_sdt=0,5)"]},{"cell_type":"markdown","metadata":{"id":"6lRRufks0qxJ"},"source":["### Model definition"]},{"cell_type":"code","metadata":{"id":"8mZLv8jGVeRR"},"source":["def resnet_block(x, filters, kernel_size=3, stride=1,\n","           conv_shortcut=False, name=None):\n","    preact = BatchNormalization(epsilon=1.001e-5, name=name + '_preact_bn')(x)\n","    preact = Activation('tanh', name=name + '_preact_tanh')(preact)\n","    if conv_shortcut is True:\n","        shortcut = Conv2D(4 * filters, 1, strides=stride, name=name + '_0_conv')(preact)\n","    else:\n","        shortcut = MaxPooling2D(1, strides=stride, name=name + 'pool_pool')(x) if stride > 1 else x\n","    x = Conv2D(filters, 1, strides=1, use_bias=False, name=name + '_1_conv')(preact)\n","    x = BatchNormalization(epsilon=1.001e-5, name=name + '_1_bn')(x)\n","    x = Activation('tanh', name=name + '_1_tanh')(x)\n","    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '_2_pad')(x)\n","    x = Conv2D(filters, kernel_size, strides=stride, use_bias=False, name=name + '_2_conv')(x)\n","    x = BatchNormalization(epsilon=1.001e-5, name=name + '_2_bn')(x)\n","    x = Activation('tanh', name=name + '_2_tanh')(x)\n","    x = Conv2D(4 * filters, 1, name=name + '_3_conv')(x)\n","    x = Add(name=name + '_out')([shortcut, x])\n","    return x\n","\n","def resnet_stack(x, filters, blocks, stride1=2, name=None):\n","    x = resnet_block(x, filters, conv_shortcut=True, name=name + '_block1')\n","    for i in range(2, blocks):\n","        x = resnet_block(x, filters, name=name + '_block' + str(i))\n","    x = resnet_block(x, filters, stride=stride1, name=name + '_block' + str(blocks))\n","    return x\n","\n","def ResNet(stack_fn, input, model_name='resnet'):\n","    # bottom\n","    x = ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(input)\n","    x = Conv2D(64, 7, strides=2, use_bias=True, name='conv1_conv')(x)\n","    x = ZeroPadding2D(padding=((1, 1), (1, 1)), name='pool1_pad')(x)\n","    x = MaxPooling2D(3, strides=2, name='pool1_pool')(x)\n","    # body\n","    x = stack_fn(x)\n","    # no top added\n","    # Create model\n","    model = tf.keras.models.Model(input, x, name=model_name)\n","    return model\n","    \n","def ResNet18_tanh(input):\n","    def stack_fn(x):\n","        x = resnet_stack(x, 64, 2, name='conv2')\n","        x = resnet_stack(x, 128, 2, name='conv3')\n","        x = resnet_stack(x, 256, 2, name='conv4')\n","        x = resnet_stack(x, 512, 2, stride1=1, name='conv5')\n","        x = BatchNormalization(epsilon=1.001e-5, name='post_bn')(x)\n","        x = Activation('tanh', name='post_tanh')(x)\n","        return x\n","    return ResNet(stack_fn, input, 'resnet18v2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cfTT_UJ0qxL"},"source":["def resnet18_builder(weights=False, dropout=False, activation='relu', name='ResNet18'):\n","    # ResNet18 without head\n","    if activation == 'tanh':\n","        resnet18 = ResNet18_tanh(Input(shape=(IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2])))\n","    elif activation == 'relu':\n","        _weights = False if weights == 'pneumonia' else weights\n","        resnet18 = ResNet18(include_top = False,\n","                            weights = _weights,\n","                            input_shape=(IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2]))\n","    \n","    resnet18.trainable = True # make layers trainable\n","\n","    seq = [\n","        InputLayer(input_shape=(IMG_SHAPE[0], IMG_SHAPE[1], IMG_SHAPE[2]), name='Input'),\n","        resnet18, # add resnet18 to model\n","        GlobalAveragePooling2D(name='AvgPool'), # add last pooling layer\n","        Dense(units=10, activation='softmax', name='Output'), # add classification layer for MNIST\n","    ]\n","\n","    if dropout:\n","        seq.insert(3, Dropout(rate=dropout, name='Dropout')) # add dropout (inspired by inception-resnet-v2))\n","\n","    model = Sequential(seq, name=name)\n","\n","    # load weights from pneumonia pretraining\n","    if weights == 'pneumonia' and activation == 'relu':\n","        model.load_weights(weights_path+'resnet18_relu_public_weights.h5', by_name=True)\n","    if weights == 'pneumonia' and activation == 'tanh':\n","        model.load_weights(weights_path+'resnet18_tanh_public_weights.h5', by_name=True)\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIvjCAkK0qxL"},"source":["resnet18 = resnet18_builder(name='ResNet18')\n","#resnet18_dropout = resnet18_builder(dropout=0.2, name='ResNet18-dropout')\n","resnet18_imagenet = resnet18_builder(weights='imagenet', name='ResNet18-imagenet')\n","#resnet18_pneumonia = resnet18_builder(weights='pneumonia', name='ResNet18-pneumonia')\n","\n","resnet18_tanh = resnet18_builder(activation='tanh', name='ResNet18-tanh')\n","#resnet18_tanh_dropout = resnet18_builder(activation='tanh', dropout=0.2, name='ResNet18-tanh-dropout')\n","#resnet18_tanh_pneumonia = resnet18_builder(activation='tanh', weights='pneumonia', name='ResNet18-tanh-pneumonia')\n","\n","models = [\n","    resnet18,\n","    #resnet18_dropout,\n","    resnet18_imagenet,\n","    #resnet18_pneumonia,\n","    resnet18_tanh,\n","    #resnet18_tanh_dropout,\n","    #resnet18_tanh_pneumonia,\n","]\n","\n","METRICS = [\n","    'accuracy',\n","    #Precision(name='precision'),\n","    #Recall(name='recall')\n","]\n","\n","for model in models:\n","    model.compile(\n","        optimizer=Adam(learning_rate=1e-3),\n","        loss='sparse_categorical_crossentropy',\n","        metrics=METRICS\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWu1DR740qxM"},"source":["models[0].summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-GisT5V0qxN"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"MOCSL8i6QoKx"},"source":["for model in models:\n","    print(\"Training %s ...\" % (model.name))\n","    history = model.fit(\n","        train_batched,\n","        steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n","        epochs=EPOCHS,\n","        callbacks = [learning_rate_decay],\n","    )\n","    print('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78_6WSB12k1x"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"L-a0_Wyy7-p2"},"source":["for model in models:\n","    print(\"Evaluating %s ...\" % (model.name))\n","    loss, acc = model.evaluate(test_batched)\n","    print('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UATHIAPIpLWQ"},"source":["### Privacy Analysis"]},{"cell_type":"markdown","metadata":{"id":"vl0MbPawMhiM"},"source":["#### Compute epsilon"]},{"cell_type":"code","metadata":{"id":"txOriPg-SAeP"},"source":["# delta rule of thumb: set to less than the inverse of the training data size\n","DELTA = 1e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYQt9IB8dJqu"},"source":["# computes epsilon for given hyperparameters\n","def compute_epsilon(n, batch_size, noise_multiplier, epochs, delta):\n","    if noise_multiplier == 0.0:\n","        return float('inf')\n","    sampling_probability = batch_size / n\n","    steps = int(np.ceil(epochs * n / batch_size))\n","    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n","    rdp = compute_rdp(\n","        q = sampling_probability,\n","        noise_multiplier = noise_multiplier,\n","        steps = steps,\n","        orders = orders)\n","    return get_privacy_spent(orders, rdp, target_delta=delta)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMxdK7NIpQAx"},"source":["eps = compute_epsilon(\n","    TRAIN_IMG_COUNT,\n","    BATCH_SIZE,\n","    NOISE_MULTIPLIER,\n","    EPOCHS,\n","    DELTA)\n","print('Delta = %.e, Epsilon = %.3f'%(DELTA, eps))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFG0vzZ-V1Yd"},"source":["#### Membership Inference Attack"]},{"cell_type":"code","metadata":{"id":"wTiPgrZvV1Yo"},"source":["def compute_attack_inputs(model, attack_data):\n","    scc = tf.keras.backend.sparse_categorical_crossentropy\n","    constant = tf.keras.backend.constant\n","    # get labels\n","    labels = []\n","    for x, y in attack_data.as_numpy_iterator():\n","        labels.append(y[0])\n","    # predict\n","    probs = model.predict(attack_data)\n","    # compute loss\n","    losses = scc(constant([[y] for y in labels]), constant(probs), from_logits=False).numpy()\n","    \n","    return (np.array(probs), np.array(losses), np.array(labels))\n","\n","# run membership inference attack\n","def run_mia(model, train_attack_input, test_attack_input):\n","    # prepare attacks\n","    probs_train, loss_train, labels_train = train_attack_input\n","    probs_test, loss_test, labels_test = test_attack_input\n","\n","    attack_input = AttackInputData(\n","        probs_train = probs_train,\n","        probs_test = probs_test,\n","        loss_train = loss_train,\n","        loss_test = loss_test,\n","        labels_train = labels_train,\n","        labels_test = labels_test\n","    )\n","\n","    slicing_spec = SlicingSpec(\n","        entire_dataset = True,\n","        by_class = True,\n","        by_percentiles = False,\n","        by_classification_correctness = True\n","    )\n","\n","    attack_types = [\n","        #AttackType.THRESHOLD_ATTACK, # logistic was always better, so no threshold to save time\n","        AttackType.LOGISTIC_REGRESSION,\n","    ] \n","\n","    # run several attacks for different data slices\n","    attacks_result = mia.run_attacks(attack_input=attack_input,\n","                                     slicing_spec=slicing_spec,\n","                                     attack_types=attack_types)\n","\n","    # plot the ROC curve of the best classifier\n","    #plotting.plot_roc_curve(attacks_result.get_result_with_max_auc().roc_curve)\n","    #plt.show()\n","\n","    # print a user-friendly summary of the attacks\n","    #print(attacks_result.summary(by_slices=False))\n","\n","    max_auc = attacks_result.get_result_with_max_auc().get_auc()\n","    max_adv = attacks_result.get_result_with_max_attacker_advantage().get_attacker_advantage()\n","\n","    return max_auc, max_adv\n","\n","# epsilon estimation based on attack, metric adapted for AUC from https://paperswithcode.com/paper/antipodes-of-label-differential-privacy-pate\n","def empirical_eps(auc):\n","    if auc <= 0.5:\n","        return 0\n","    if auc == 1:\n","        return np.inf\n","    return np.log(auc / (1 - auc))\n","\n","from contextlib import contextmanager\n","import sys\n","@contextmanager\n","def suppress_stdout():\n","    with open(os.devnull, \"w\") as devnull:\n","        old_stdout = sys.stdout\n","        sys.stdout = devnull\n","        try:  \n","            yield\n","        finally:\n","            sys.stdout = old_stdout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNR4ieF7V1Yp"},"source":["import scipy.stats as st\n","\n","for model in models:\n","    print('Membership Inference Attack on '+model.name+'...')\n","    SAMPLE_NB = 100\n","    \n","    train_attack_input = compute_attack_inputs(model, train_attack_data)\n","    test_attack_input = compute_attack_inputs(model, test_attack_data)\n","\n","    aucs = []\n","    advs = []\n","    emp_eps = []\n","    with suppress_stdout():\n","        for i in range(SAMPLE_NB):\n","            max_auc, max_adv = run_mia(model, train_attack_input, test_attack_input)\n","            aucs.append(max_auc)\n","            advs.append(max_adv)\n","            emp_eps.append(empirical_eps(max_auc))\n","\n","        auc_low, auc_high = st.t.interval(0.95, len(aucs)-1, loc=np.mean(aucs), scale=st.sem(aucs))\n","        adv_low, adv_high = st.t.interval(0.95, len(advs)-1, loc=np.mean(advs), scale=st.sem(advs))\n","        eps_low, eps_high = st.t.interval(0.95, len(emp_eps)-1, loc=np.mean(emp_eps), scale=st.sem(emp_eps))\n","\n","    print('95%%-CI based on %i attack samples'%(SAMPLE_NB))\n","    print('AUC: %0.2f-%0.2f'%(auc_low, auc_high))\n","    print('Attacker advantage: %0.2f-%0.2f'%(adv_low, adv_high))\n","    print('Empirical epsilon bounds from AUC: %0.2f-%0.2f\\n'%(eps_low, eps_high))"],"execution_count":null,"outputs":[]}]}